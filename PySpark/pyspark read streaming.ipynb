{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import explode\n",
    "from pyspark.sql.functions import split\n",
    "\n",
    "spark =SparkSession.builder.appName('appname').getOrCreate()\n",
    "# Create DataFrame representing the stream of input lines from connection to localhost:9999\n",
    "streamingDF = spark.readStream\\\n",
    "                .format('socket')\\\n",
    "                .option('host','localhost')\\\n",
    "                .option('port',9999)\\\n",
    "                .load()\n",
    "# Split the lines into words\n",
    "words = streamingDF.select(\n",
    "        explode(\n",
    "            split(streamingDF.value, ' ')\n",
    "        ).alias('word')\n",
    ")\n",
    "# Generate running word count\n",
    "wordCounts = words.groupBy('word').count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col\n",
    "\n",
    "# Step 1: Initialize Spark Session\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"Streaming Transformation Example\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "# Step 2: Define the Streaming Source\n",
    "# Assuming the stream is coming from a directory with continuous input files\n",
    "input_stream = spark.readStream \\\n",
    "    .format(\"json\") \\\n",
    "    .option(\"path\", \"path/to/input/directory\") \\\n",
    "    .option(\"maxFilesPerTrigger\", 1) \\\n",
    "    .load()\n",
    "\n",
    "# Step 3: Define the Transformation\n",
    "# Example transformation: filtering the data\n",
    "transformed_stream = input_stream.filter(col(\"someColumn\") > 10)\n",
    "\n",
    "# Step 4: Output the Transformed Data\n",
    "# Writing the results to the console\n",
    "query = transformed_stream.writeStream \\\n",
    "    .outputMode(\"append\") \\\n",
    "    .format(\"console\") \\\n",
    "    .start()\n",
    "\n",
    "# Start the stream and wait for it to finish\n",
    "query.awaitTermination()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col\n",
    "\n",
    "# Create a Spark session\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"Filter Streaming Data\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "# Read the streaming data from a source\n",
    "streamingDF = spark.readStream \\\n",
    "    .format(\"socket\") \\\n",
    "    .option(\"host\", \"localhost\") \\\n",
    "    .option(\"port\", 9999) \\\n",
    "    .load()\n",
    "\n",
    "# Apply a filter transformation\n",
    "filteredDF = streamingDF.filter(col(\"value\") > 100)\n",
    "\n",
    "# Write the results to the console\n",
    "query = filteredDF.writeStream \\\n",
    "    .outputMode(\"append\") \\\n",
    "    .format(\"console\") \\\n",
    "    .start()\n",
    "\n",
    "query.awaitTermination()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import *\n",
    "\n",
    "# Create a Spark session\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"Aggregate Streaming Data\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "# Read the streaming data from a source\n",
    "streamingDF = spark.readStream \\\n",
    "    .format(\"socket\") \\\n",
    "    .option(\"host\", \"localhost\") \\\n",
    "    .option(\"port\", 9999) \\\n",
    "    .load()\n",
    "\n",
    "# Define aggregation\n",
    "aggregatedDF = streamingDF.groupBy(\"someGroupingColumn\").count()\n",
    "\n",
    "# Write the results to the console\n",
    "query = aggregatedDF.writeStream \\\n",
    "    .outputMode(\"complete\") \\\n",
    "    .format(\"console\") \\\n",
    "    .start()\n",
    "\n",
    "query.awaitTermination()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# Create a Spark session\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"Join Stream with Static Data\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "# Static DataFrame\n",
    "staticDF = spark.read.json(\"path/to/static/data.json\")\n",
    "\n",
    "# Streaming DataFrame\n",
    "streamingDF = spark.readStream \\\n",
    "    .format(\"socket\") \\\n",
    "    .option(\"host\", \"localhost\") \\\n",
    "    .option(\"port\", 9999) \\\n",
    "    .load()\n",
    "\n",
    "# Join operation\n",
    "joinedDF = streamingDF.join(staticDF, \"key\")\n",
    "\n",
    "# Write the results to the console\n",
    "query = joinedDF.writeStream \\\n",
    "    .outputMode(\"append\") \\\n",
    "    .format(\"console\") \\\n",
    "    .start()\n",
    "\n",
    "query.awaitTermination()\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
