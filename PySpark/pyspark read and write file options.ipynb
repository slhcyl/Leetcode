{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# Initialize a Spark session\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"Read Parquet Example\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "# Path to your Parquet file\n",
    "parquet_file_path = 'path_to_parquet_file.parquet'\n",
    "\n",
    "# Read the Parquet file\n",
    "df_parquet = spark.read.parquet(parquet_file_path)\n",
    "\n",
    "# Show the first few rows of the DataFrame\n",
    "df_parquet.show()\n",
    "\n",
    "# Stop the Spark session\n",
    "spark.stop()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merging schemas: If you're reading multiple Parquet files with different but compatible schemas, you can merge their schemas.\n",
    "df = spark.read.option(\"mergeSchema\", \"true\").parquet(\"path/to/different/schemas/\")\n",
    "# Filtering files: You can filter which Parquet files to read directly in the read.parquet call by specifying a pattern or a subset of files.\n",
    "df = spark.read.parquet(\"path/to/parquet/files/part-*.parquet\")\n",
    "# Reading specific columns: You can specify only certain columns to load if you do not need all the data, which can improve performance by reducing I/O.\n",
    "df = spark.read.parquet(\"path/to/parquet/file.parquet\").select(\"column1\", \"column2\")\n",
    "# Read the Parquet file using read.format()\n",
    "df_parquet = spark.read.format(\"parquet\").load(parquet_file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder.appName('test').getOrCreate()\n",
    "parquet_file_path = ''\n",
    "df = spark.read.parquet(parquet_file_path)\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# Initialize a Spark session\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"Read Multiple Parquet Files\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "# Path to your directory containing Parquet files\n",
    "parquet_directory_path = 'path_to_parquet_directory/'\n",
    "\n",
    "# Read all Parquet files in the directory\n",
    "df_parquet = spark.read.parquet(parquet_directory_path + '*')\n",
    "\n",
    "# Show the first few rows of the DataFrame\n",
    "df_parquet.show()\n",
    "\n",
    "# Stop the Spark session\n",
    "spark.stop()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# Initialize a Spark session\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"Read Parquet Format Example\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "# Path to your Parquet file\n",
    "parquet_file_path = 'path_to_parquet_file.parquet'\n",
    "\n",
    "# Read the Parquet file using read.format()\n",
    "df_parquet = spark.read.format(\"parquet\").load(parquet_file_path)\n",
    "\n",
    "# Show the first few rows of the DataFrame\n",
    "df_parquet.show()\n",
    "\n",
    "# Stop the Spark session\n",
    "spark.stop()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.read.format('parquet').load(parquet_file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import StructType, StructField, StringType, IntegerType\n",
    "\n",
    "# Initialize a Spark session\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"Read Parquet with Schema\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "# Define your schema\n",
    "schema = StructType([\n",
    "    StructField(\"Name\", StringType(), True),\n",
    "    StructField(\"Age\", IntegerType(), True)\n",
    "])\n",
    "\n",
    "# Path to your Parquet file\n",
    "parquet_file_path = 'path_to_parquet_file.parquet'\n",
    "\n",
    "# Read the Parquet file with a predefined schema\n",
    "df_parquet = spark.read.schema(schema).parquet(parquet_file_path)\n",
    "\n",
    "# Show the first few rows of the DataFrame\n",
    "df_parquet.show()\n",
    "\n",
    "# Stop the Spark session\n",
    "spark.stop()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# Initialize a Spark session\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"Write Parquet Example\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "# Example DataFrame creation (you might have your DataFrame ready from previous operations)\n",
    "data = [(\"James\", 34), (\"Anna\", 28), (\"Lee\", 23)]\n",
    "columns = [\"Name\", \"Age\"]\n",
    "df = spark.createDataFrame(data, schema=columns)\n",
    "\n",
    "# Path to output the Parquet file\n",
    "output_parquet_path = 'output_path.parquet'\n",
    "\n",
    "# Write the DataFrame to a Parquet file\n",
    "df.write.parquet(output_parquet_path, mode='overwrite')\n",
    "\n",
    "# Stop the Spark session\n",
    "spark.stop()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assume 'df' is your DataFrame\n",
    "\n",
    "# To save the DataFrame as a Parquet file\n",
    "df.write.format(\"parquet\").save(\"path/to/output/file.parquet\")\n",
    "# This is equivalent to the previous example\n",
    "df.write.parquet(\"path/to/output/file.parquet\")\n",
    "df.write.parquet(output_parquet_path, mode='overwrite')\n",
    "df.write.format(\"parquet\").option(\"compression\", \"snappy\").save(\"path/to/output/file.parquet\")\n",
    "df.write.format(\"parquet\").mode(\"overwrite\").save(\"path/to/output/file.parquet\")\n",
    "df.write.format(\"parquet\").partitionBy(\"column_name\").save(\"path/to/output/file.parquet\")\n",
    "df.write.format(\"parquet\").option(\"path\", \"path/to/output/file.parquet\").save()\n",
    "df.write.format(\"parquet\").option(\"mergeSchema\", \"true\").save(\"path/to/output/file.parquet\")\n",
    "df.write.format(\"parquet\")\\\n",
    "    .option(\"compression\", \"snappy\")\\\n",
    "    .option(\"mergeSchema\", \"true\")\\\n",
    "    .partitionBy(\"column_name\")\\\n",
    "    .mode(\"overwrite\")\\\n",
    "    .save(\"path/to/output/file.parquet\")\n",
    "# To write a DataFrame to a Parquet file with a specific mode\n",
    "df.write.mode(\"overwrite\").parquet(\"path/to/output/file.parquet\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# Initialize a Spark session\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"Read CSV Example\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "# Path to your CSV file\n",
    "csv_file_path = 'path_to_csv_file.csv'\n",
    "\n",
    "# Read the CSV file\n",
    "df_csv = spark.read.format(\"csv\") \\\n",
    "    .option(\"header\", \"true\")  # Assumes the first row is a header\n",
    "    .option(\"inferSchema\", \"true\")  # Infers the input schema automatically from data\n",
    "    .load(csv_file_path)\n",
    "\n",
    "# Show the first few rows of the DataFrame\n",
    "df_csv.show()\n",
    "\n",
    "# Stop the Spark session\n",
    "spark.stop()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# Initialize a Spark session\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"Read Multiple CSV Files\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "# Multiple CSV file paths\n",
    "csv_file_path1 = 'path_to_first_csv_file.csv'\n",
    "csv_file_path2 = 'path_to_second_csv_file.csv'\n",
    "\n",
    "# Read multiple CSV files\n",
    "df_csv = spark.read.format(\"csv\") \\\n",
    "    .option(\"header\", \"true\") \\\n",
    "    .option(\"inferSchema\", \"true\") \\\n",
    "    .load(csv_file_path1, csv_file_path2)\n",
    "\n",
    "# Show the first few rows of the DataFrame\n",
    "df_csv.show()\n",
    "\n",
    "# Stop the Spark session\n",
    "spark.stop()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# Initialize a Spark session\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"Read CSV Files with Wildcard\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "# Directory path containing CSV files\n",
    "directory_path = 'path_to_csv_directory/'\n",
    "\n",
    "# Read all CSV files in the directory that match a pattern\n",
    "df_csv = spark.read.format(\"csv\") \\\n",
    "    .option(\"header\", \"true\") \\\n",
    "    .option(\"inferSchema\", \"true\") \\\n",
    "    .load(directory_path + \"*.csv\")  # Adjust pattern as needed\n",
    "\n",
    "# Show the first few rows of the DataFrame\n",
    "df_csv.show()\n",
    "\n",
    "# Stop the Spark session\n",
    "spark.stop()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# Initialize a Spark session\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"Read All CSV Files in Directory\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "# Directory path containing CSV files\n",
    "directory_path = 'path_to_csv_directory/'\n",
    "\n",
    "# Read all CSV files in the directory\n",
    "df_csv = spark.read.csv(directory_path, header=True, inferSchema=True)\n",
    "\n",
    "# Show the first few rows of the DataFrame\n",
    "df_csv.show()\n",
    "\n",
    "# Stop the Spark session\n",
    "spark.stop()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# Initialize a Spark session\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"Read Single CSV File\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "# Path to your CSV file\n",
    "csv_file_path = 'path_to_csv_file.csv'\n",
    "\n",
    "# Read the CSV file\n",
    "df_csv = spark.read.csv(csv_file_path, header=True, inferSchema=True)\n",
    "\n",
    "# Show the first few rows of the DataFrame\n",
    "df_csv.show()\n",
    "\n",
    "# Stop the Spark session\n",
    "spark.stop()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# Initialize a Spark session\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"Write CSV Example\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "# Example DataFrame creation (you might have your DataFrame ready from previous operations)\n",
    "data = [(\"James\", 34), (\"Anna\", 28), (\"Lee\", 23)]\n",
    "columns = [\"Name\", \"Age\"]\n",
    "df = spark.createDataFrame(data, schema=columns)\n",
    "\n",
    "# Path to output the CSV file\n",
    "output_csv_path = 'output_path.csv'\n",
    "\n",
    "# Write the DataFrame to a CSV file\n",
    "df.write.format(\"csv\") \\\n",
    "    .option(\"header\", \"true\") \\\n",
    "    .mode(\"overwrite\") \\\n",
    "    .save(output_csv_path)\n",
    "\n",
    "# Stop the Spark session\n",
    "spark.stop()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.write.format(\"csv\").option(\"delimiter\", \";\").save(\"path/to/output/file.csv\")\n",
    "df.write.format(\"csv\").option(\"header\", \"true\").save(\"path/to/output/file.csv\")\n",
    "df.write.format(\"csv\").option(\"quote\", \"\\\"\").save(\"path/to/output/file.csv\")\n",
    "df.write.format(\"csv\").option(\"escape\", \"\\\\\").save(\"path/to/output/file.csv\")\n",
    "df.write.format(\"csv\").option(\"nullValue\", \"NULL\").save(\"path/to/output/file.csv\")\n",
    "df.write.format(\"csv\").option(\"nullValue\", \"NULL\").save(\"path/to/output/file.csv\")\n",
    "df.write.format(\"csv\").option(\"codec\", \"gzip\").save(\"path/to/output/file.csv\")\n",
    "df.write.format(\"csv\").mode(\"overwrite\").save(\"path/to/output/file.csv\")\n",
    "df.write.format(\"csv\").option(\"quoteAll\", \"true\").save(\"path/to/output/file.csv\")\n",
    "df.write.format(\"csv\")\\\n",
    "    .option(\"delimiter\", \";\")\\\n",
    "    .option(\"header\", \"true\")\\\n",
    "    .option(\"nullValue\", \"NULL\")\\\n",
    "    .option(\"codec\", \"gzip\")\\\n",
    "    .mode(\"overwrite\")\\\n",
    "    .save(\"path/to/output/file.csv\")\\\n",
    "# To write a DataFrame to a CSV file with a specific mode\n",
    "df.write.mode(\"overwrite\").csv(\"path/to/output/file.csv\")\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
