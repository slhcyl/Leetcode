{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# Initialize a Spark session\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"Read Parquet Example\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "# Path to your Parquet file\n",
    "parquet_file_path = 'path_to_parquet_file.parquet'\n",
    "\n",
    "# Read the Parquet file\n",
    "df_parquet = spark.read.parquet(parquet_file_path)\n",
    "\n",
    "# Show the first few rows of the DataFrame\n",
    "df_parquet.show()\n",
    "\n",
    "# Stop the Spark session\n",
    "spark.stop()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# Initialize a Spark session\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"Read Multiple Parquet Files\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "# Path to your directory containing Parquet files\n",
    "parquet_directory_path = 'path_to_parquet_directory/'\n",
    "\n",
    "# Read all Parquet files in the directory\n",
    "df_parquet = spark.read.parquet(parquet_directory_path + '*')\n",
    "\n",
    "# Show the first few rows of the DataFrame\n",
    "df_parquet.show()\n",
    "\n",
    "# Stop the Spark session\n",
    "spark.stop()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# Initialize a Spark session\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"Read Parquet Format Example\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "# Path to your Parquet file\n",
    "parquet_file_path = 'path_to_parquet_file.parquet'\n",
    "\n",
    "# Read the Parquet file using read.format()\n",
    "df_parquet = spark.read.format(\"parquet\").load(parquet_file_path)\n",
    "\n",
    "# Show the first few rows of the DataFrame\n",
    "df_parquet.show()\n",
    "\n",
    "# Stop the Spark session\n",
    "spark.stop()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import StructType, StructField, StringType, IntegerType\n",
    "\n",
    "# Initialize a Spark session\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"Read Parquet with Schema\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "# Define your schema\n",
    "schema = StructType([\n",
    "    StructField(\"Name\", StringType(), True),\n",
    "    StructField(\"Age\", IntegerType(), True)\n",
    "])\n",
    "\n",
    "# Path to your Parquet file\n",
    "parquet_file_path = 'path_to_parquet_file.parquet'\n",
    "\n",
    "# Read the Parquet file with a predefined schema\n",
    "df_parquet = spark.read.schema(schema).parquet(parquet_file_path)\n",
    "\n",
    "# Show the first few rows of the DataFrame\n",
    "df_parquet.show()\n",
    "\n",
    "# Stop the Spark session\n",
    "spark.stop()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# Initialize a Spark session\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"Write Parquet Example\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "# Example DataFrame creation (you might have your DataFrame ready from previous operations)\n",
    "data = [(\"James\", 34), (\"Anna\", 28), (\"Lee\", 23)]\n",
    "columns = [\"Name\", \"Age\"]\n",
    "df = spark.createDataFrame(data, schema=columns)\n",
    "\n",
    "# Path to output the Parquet file\n",
    "output_parquet_path = 'output_path.parquet'\n",
    "\n",
    "# Write the DataFrame to a Parquet file\n",
    "df.write.parquet(output_parquet_path, mode='overwrite')\n",
    "\n",
    "# Stop the Spark session\n",
    "spark.stop()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# Initialize a Spark session\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"Read CSV Example\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "# Path to your CSV file\n",
    "csv_file_path = 'path_to_csv_file.csv'\n",
    "\n",
    "# Read the CSV file\n",
    "df_csv = spark.read.format(\"csv\") \\\n",
    "    .option(\"header\", \"true\")  # Assumes the first row is a header\n",
    "    .option(\"inferSchema\", \"true\")  # Infers the input schema automatically from data\n",
    "    .load(csv_file_path)\n",
    "\n",
    "# Show the first few rows of the DataFrame\n",
    "df_csv.show()\n",
    "\n",
    "# Stop the Spark session\n",
    "spark.stop()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# Initialize a Spark session\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"Read Multiple CSV Files\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "# Multiple CSV file paths\n",
    "csv_file_path1 = 'path_to_first_csv_file.csv'\n",
    "csv_file_path2 = 'path_to_second_csv_file.csv'\n",
    "\n",
    "# Read multiple CSV files\n",
    "df_csv = spark.read.format(\"csv\") \\\n",
    "    .option(\"header\", \"true\") \\\n",
    "    .option(\"inferSchema\", \"true\") \\\n",
    "    .load(csv_file_path1, csv_file_path2)\n",
    "\n",
    "# Show the first few rows of the DataFrame\n",
    "df_csv.show()\n",
    "\n",
    "# Stop the Spark session\n",
    "spark.stop()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# Initialize a Spark session\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"Read CSV Files with Wildcard\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "# Directory path containing CSV files\n",
    "directory_path = 'path_to_csv_directory/'\n",
    "\n",
    "# Read all CSV files in the directory that match a pattern\n",
    "df_csv = spark.read.format(\"csv\") \\\n",
    "    .option(\"header\", \"true\") \\\n",
    "    .option(\"inferSchema\", \"true\") \\\n",
    "    .load(directory_path + \"*.csv\")  # Adjust pattern as needed\n",
    "\n",
    "# Show the first few rows of the DataFrame\n",
    "df_csv.show()\n",
    "\n",
    "# Stop the Spark session\n",
    "spark.stop()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# Initialize a Spark session\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"Read All CSV Files in Directory\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "# Directory path containing CSV files\n",
    "directory_path = 'path_to_csv_directory/'\n",
    "\n",
    "# Read all CSV files in the directory\n",
    "df_csv = spark.read.csv(directory_path, header=True, inferSchema=True)\n",
    "\n",
    "# Show the first few rows of the DataFrame\n",
    "df_csv.show()\n",
    "\n",
    "# Stop the Spark session\n",
    "spark.stop()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# Initialize a Spark session\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"Read Single CSV File\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "# Path to your CSV file\n",
    "csv_file_path = 'path_to_csv_file.csv'\n",
    "\n",
    "# Read the CSV file\n",
    "df_csv = spark.read.csv(csv_file_path, header=True, inferSchema=True)\n",
    "\n",
    "# Show the first few rows of the DataFrame\n",
    "df_csv.show()\n",
    "\n",
    "# Stop the Spark session\n",
    "spark.stop()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# Initialize a Spark session\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"Write CSV Example\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "# Example DataFrame creation (you might have your DataFrame ready from previous operations)\n",
    "data = [(\"James\", 34), (\"Anna\", 28), (\"Lee\", 23)]\n",
    "columns = [\"Name\", \"Age\"]\n",
    "df = spark.createDataFrame(data, schema=columns)\n",
    "\n",
    "# Path to output the CSV file\n",
    "output_csv_path = 'output_path.csv'\n",
    "\n",
    "# Write the DataFrame to a CSV file\n",
    "df.write.format(\"csv\") \\\n",
    "    .option(\"header\", \"true\") \\\n",
    "    .mode(\"overwrite\") \\\n",
    "    .save(output_csv_path)\n",
    "\n",
    "# Stop the Spark session\n",
    "spark.stop()\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
