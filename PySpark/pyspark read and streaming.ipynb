{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import explode\n",
    "from pyspark.sql.functions import split\n",
    "\n",
    "spark =SparkSession.builder.appName('appname').getOrCreate()\n",
    "# Create DataFrame representing the stream of input lines from connection to localhost:9999\n",
    "streamingDF = spark.readStream\\\n",
    "                .format('socket')\\\n",
    "                .option('host','localhost')\\\n",
    "                .option('port',9999)\\\n",
    "                .load()\n",
    "# Split the lines into words\n",
    "words = streamingDF.select(\n",
    "        explode(\n",
    "            split(streamingDF.value, ' ')\n",
    "        ).alias('word')\n",
    ")\n",
    "# Generate running word count\n",
    "wordCounts = words.groupBy('word').count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.readStream.format('socket').option('host','localhost').option('port',9999).load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col\n",
    "\n",
    "# Step 1: Initialize Spark Session\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"Streaming Transformation Example\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "# Step 2: Define the Streaming Source\n",
    "# Assuming the stream is coming from a directory with continuous input files\n",
    "input_stream = spark.readStream \\\n",
    "    .format(\"json\") \\\n",
    "    .option(\"path\", \"path/to/input/directory\") \\\n",
    "    .option(\"maxFilesPerTrigger\", 1) \\\n",
    "    .load()\n",
    "\n",
    "# Step 3: Define the Transformation\n",
    "# Example transformation: filtering the data\n",
    "transformed_stream = input_stream.filter(col(\"someColumn\") > 10)\n",
    "\n",
    "# Step 4: Output the Transformed Data\n",
    "# Writing the results to the console\n",
    "query = transformed_stream.writeStream \\\n",
    "    .outputMode(\"append\") \\\n",
    "    .format(\"console\") \\\n",
    "    .start()\n",
    "\n",
    "# Start the stream and wait for it to finish\n",
    "query.awaitTermination()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col\n",
    "\n",
    "# Create a Spark session\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"Filter Streaming Data\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "# Read the streaming data from a source\n",
    "streamingDF = spark.readStream \\\n",
    "    .format(\"socket\") \\\n",
    "    .option(\"host\", \"localhost\") \\\n",
    "    .option(\"port\", 9999) \\\n",
    "    .load()\n",
    "\n",
    "# Apply a filter transformation\n",
    "filteredDF = streamingDF.filter(col(\"value\") > 100)\n",
    "\n",
    "# Write the results to the console\n",
    "query = filteredDF.writeStream \\\n",
    "    .outputMode(\"append\") \\\n",
    "    .format(\"console\") \\\n",
    "    .start()\n",
    "\n",
    "query.awaitTermination()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import *\n",
    "\n",
    "# Create a Spark session\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"Aggregate Streaming Data\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "# Read the streaming data from a source\n",
    "streamingDF = spark.readStream \\\n",
    "    .format(\"socket\") \\\n",
    "    .option(\"host\", \"localhost\") \\\n",
    "    .option(\"port\", 9999) \\\n",
    "    .load()\n",
    "\n",
    "# Define aggregation\n",
    "aggregatedDF = streamingDF.groupBy(\"someGroupingColumn\").count()\n",
    "\n",
    "# Write the results to the console\n",
    "query = aggregatedDF.writeStream \\\n",
    "    .outputMode(\"complete\") \\\n",
    "    .format(\"console\") \\\n",
    "    .start()\n",
    "\n",
    "query.awaitTermination()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you want to read streaming data from a website, you generally need that website to provide data in a stream-friendly format, such as through a WebSocket or continuously open HTTP connection. However, Spark itself doesn't natively support reading directly from WebSockets or similar protocols without additional implementations or tools.\n",
    "\n",
    "A common approach within Spark’s ecosystem is to use Apache Kafka as an intermediary, where the website pushes data to a Kafka topic, and Spark reads from this Kafka topic. However, Spark can directly read data streams over TCP sockets, which is useful for simple streaming applications or for learning purposes.\n",
    "\n",
    "Here’s an example of how you might set up a simple stream reader in Spark to read data from a TCP socket:\n",
    "In this setup, you'd need a server on localhost running on port 9999 sending data. For production use, replace \"localhost\" and \"9999\" with the appropriate IP address and port of your data source."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# Initialize Spark Session for Structured Streaming\n",
    "spark = SparkSession.builder.appName(\"Socket Stream Example\").getMaster(\"local\").getOrCreate()\n",
    "\n",
    "# Create DataFrame representing the stream of input lines from connection to localhost:9999\n",
    "lines = spark.readStream.format(\"socket\")\\\n",
    "    .option(\"host\", \"localhost\")\\\n",
    "    .option(\"port\", 9999).load()\n",
    "\n",
    "# This DataFrame now represents an unbounded table containing the streaming text data\n",
    "\n",
    "# Assuming 'lines' is a DataFrame created from readStream as shown above\n",
    "query = lines.writeStream.outputMode(\"append\").format(\"console\").start()\n",
    "\n",
    "# Start running the query that prints the output to the console\n",
    "query.awaitTermination()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# Create a Spark session\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"Join Stream with Static Data\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "# Static DataFrame\n",
    "staticDF = spark.read.json(\"path/to/static/data.json\")\n",
    "\n",
    "# Streaming DataFrame\n",
    "streamingDF = spark.readStream \\\n",
    "    .format(\"socket\") \\\n",
    "    .option(\"host\", \"localhost\") \\\n",
    "    .option(\"port\", 9999) \\\n",
    "    .load()\n",
    "\n",
    "# Join operation\n",
    "joinedDF = streamingDF.join(staticDF, \"key\")\n",
    "\n",
    "# Write the results to the console\n",
    "query = joinedDF.writeStream \\\n",
    "    .outputMode(\"append\") \\\n",
    "    .format(\"console\") \\\n",
    "    .start()\n",
    "\n",
    "query.awaitTermination()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"File Streaming\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "# Read streaming data from a directory\n",
    "streamingDF = spark.readStream \\\n",
    "    .format(\"csv\") \\\n",
    "    .option(\"header\", \"true\") \\\n",
    "    .schema(your_schema)  # Define the schema of the CSV files\n",
    "    .load(\"path_to_directory_containing_csv_files\")\n",
    "\n",
    "# Query to test the stream\n",
    "query = streamingDF.writeStream \\\n",
    "    .outputMode(\"append\") \\\n",
    "    .format(\"console\") \\\n",
    "    .start()\n",
    "\n",
    "query.awaitTermination()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"Kafka Streaming\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "# Read streaming data from Kafka\n",
    "kafkaDF = spark.readStream \\\n",
    "    .format(\"kafka\") \\\n",
    "    .option(\"kafka.bootstrap.servers\", \"host1:port1,host2:port2\") \\\n",
    "    .option(\"subscribe\", \"topic_name\") \\\n",
    "    .load()\n",
    "\n",
    "# Query to test the stream\n",
    "query = kafkaDF.writeStream \\\n",
    "    .outputMode(\"append\") \\\n",
    "    .format(\"console\") \\\n",
    "    .start()\n",
    "\n",
    "query.awaitTermination()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import StructType\n",
    "\n",
    "# Initialize a Spark session\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"Parquet Streaming\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "# Define the schema of the Parquet files (optional but recommended for performance)\n",
    "schema = StructType([...])  # Define your schema here based on the Parquet file structure\n",
    "\n",
    "# Directory containing incoming Parquet files\n",
    "input_directory = 'path_to_input_directory/'\n",
    "\n",
    "# Read streaming data from Parquet files\n",
    "parquetStream = spark.readStream \\\n",
    "    .format(\"parquet\") \\\n",
    "    .schema(schema)  # Use this if you want to specify the schema, or remove it to infer schema\n",
    "    .load(input_directory)\n",
    "\n",
    "# Define a simple query to process and output the streamed data\n",
    "query = parquetStream.writeStream \\\n",
    "    .outputMode(\"append\") \\\n",
    "    .format(\"console\") \\\n",
    "    .start()  # Outputs to the console for demonstration; replace with other sinks as needed\n",
    "\n",
    "query.awaitTermination()\n",
    "\n",
    "# Stop the Spark session\n",
    "spark.stop()\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
