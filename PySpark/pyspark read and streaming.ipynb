{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import explode\n",
    "from pyspark.sql.functions import split\n",
    "\n",
    "spark =SparkSession.builder.appName('appname').getOrCreate()\n",
    "# Create DataFrame representing the stream of input lines from connection to localhost:9999\n",
    "streamingDF = spark.readStream\\\n",
    "                .format('socket')\\\n",
    "                .option('host','localhost')\\\n",
    "                .option('port',9999)\\\n",
    "                .load()\n",
    "# Split the lines into words\n",
    "words = streamingDF.select(\n",
    "        explode(\n",
    "            split(streamingDF.value, ' ')\n",
    "        ).alias('word')\n",
    ")\n",
    "# Generate running word count\n",
    "wordCounts = words.groupBy('word').count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col\n",
    "\n",
    "# Step 1: Initialize Spark Session\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"Streaming Transformation Example\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "# Step 2: Define the Streaming Source\n",
    "# Assuming the stream is coming from a directory with continuous input files\n",
    "input_stream = spark.readStream \\\n",
    "    .format(\"json\") \\\n",
    "    .option(\"path\", \"path/to/input/directory\") \\\n",
    "    .option(\"maxFilesPerTrigger\", 1) \\\n",
    "    .load()\n",
    "\n",
    "# Step 3: Define the Transformation\n",
    "# Example transformation: filtering the data\n",
    "transformed_stream = input_stream.filter(col(\"someColumn\") > 10)\n",
    "\n",
    "# Step 4: Output the Transformed Data\n",
    "# Writing the results to the console\n",
    "query = transformed_stream.writeStream \\\n",
    "    .outputMode(\"append\") \\\n",
    "    .format(\"console\") \\\n",
    "    .start()\n",
    "\n",
    "# Start the stream and wait for it to finish\n",
    "query.awaitTermination()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col\n",
    "\n",
    "# Create a Spark session\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"Filter Streaming Data\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "# Read the streaming data from a source\n",
    "streamingDF = spark.readStream \\\n",
    "    .format(\"socket\") \\\n",
    "    .option(\"host\", \"localhost\") \\\n",
    "    .option(\"port\", 9999) \\\n",
    "    .load()\n",
    "\n",
    "# Apply a filter transformation\n",
    "filteredDF = streamingDF.filter(col(\"value\") > 100)\n",
    "\n",
    "# Write the results to the console\n",
    "query = filteredDF.writeStream \\\n",
    "    .outputMode(\"append\") \\\n",
    "    .format(\"console\") \\\n",
    "    .start()\n",
    "\n",
    "query.awaitTermination()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import *\n",
    "\n",
    "# Create a Spark session\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"Aggregate Streaming Data\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "# Read the streaming data from a source\n",
    "streamingDF = spark.readStream \\\n",
    "    .format(\"socket\") \\\n",
    "    .option(\"host\", \"localhost\") \\\n",
    "    .option(\"port\", 9999) \\\n",
    "    .load()\n",
    "\n",
    "# Define aggregation\n",
    "aggregatedDF = streamingDF.groupBy(\"someGroupingColumn\").count()\n",
    "\n",
    "# Write the results to the console\n",
    "query = aggregatedDF.writeStream \\\n",
    "    .outputMode(\"complete\") \\\n",
    "    .format(\"console\") \\\n",
    "    .start()\n",
    "\n",
    "query.awaitTermination()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# Create a Spark session\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"Join Stream with Static Data\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "# Static DataFrame\n",
    "staticDF = spark.read.json(\"path/to/static/data.json\")\n",
    "\n",
    "# Streaming DataFrame\n",
    "streamingDF = spark.readStream \\\n",
    "    .format(\"socket\") \\\n",
    "    .option(\"host\", \"localhost\") \\\n",
    "    .option(\"port\", 9999) \\\n",
    "    .load()\n",
    "\n",
    "# Join operation\n",
    "joinedDF = streamingDF.join(staticDF, \"key\")\n",
    "\n",
    "# Write the results to the console\n",
    "query = joinedDF.writeStream \\\n",
    "    .outputMode(\"append\") \\\n",
    "    .format(\"console\") \\\n",
    "    .start()\n",
    "\n",
    "query.awaitTermination()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"File Streaming\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "# Read streaming data from a directory\n",
    "streamingDF = spark.readStream \\\n",
    "    .format(\"csv\") \\\n",
    "    .option(\"header\", \"true\") \\\n",
    "    .schema(your_schema)  # Define the schema of the CSV files\n",
    "    .load(\"path_to_directory_containing_csv_files\")\n",
    "\n",
    "# Query to test the stream\n",
    "query = streamingDF.writeStream \\\n",
    "    .outputMode(\"append\") \\\n",
    "    .format(\"console\") \\\n",
    "    .start()\n",
    "\n",
    "query.awaitTermination()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"Kafka Streaming\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "# Read streaming data from Kafka\n",
    "kafkaDF = spark.readStream \\\n",
    "    .format(\"kafka\") \\\n",
    "    .option(\"kafka.bootstrap.servers\", \"host1:port1,host2:port2\") \\\n",
    "    .option(\"subscribe\", \"topic_name\") \\\n",
    "    .load()\n",
    "\n",
    "# Query to test the stream\n",
    "query = kafkaDF.writeStream \\\n",
    "    .outputMode(\"append\") \\\n",
    "    .format(\"console\") \\\n",
    "    .start()\n",
    "\n",
    "query.awaitTermination()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import StructType\n",
    "\n",
    "# Initialize a Spark session\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"Parquet Streaming\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "# Define the schema of the Parquet files (optional but recommended for performance)\n",
    "schema = StructType([...])  # Define your schema here based on the Parquet file structure\n",
    "\n",
    "# Directory containing incoming Parquet files\n",
    "input_directory = 'path_to_input_directory/'\n",
    "\n",
    "# Read streaming data from Parquet files\n",
    "parquetStream = spark.readStream \\\n",
    "    .format(\"parquet\") \\\n",
    "    .schema(schema)  # Use this if you want to specify the schema, or remove it to infer schema\n",
    "    .load(input_directory)\n",
    "\n",
    "# Define a simple query to process and output the streamed data\n",
    "query = parquetStream.writeStream \\\n",
    "    .outputMode(\"append\") \\\n",
    "    .format(\"console\") \\\n",
    "    .start()  # Outputs to the console for demonstration; replace with other sinks as needed\n",
    "\n",
    "query.awaitTermination()\n",
    "\n",
    "# Stop the Spark session\n",
    "spark.stop()\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
